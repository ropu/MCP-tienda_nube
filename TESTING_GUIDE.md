# GuÃ­a de EjecuciÃ³n de Pruebas - Herramientas MCP

## ğŸ“‹ Resumen

Este documento describe cÃ³mo ejecutar las pruebas exhaustivas para los 8 endpoints de herramientas MCP del servidor FastAPI.

**Total de Casos de Prueba:** 158
- Casos de Ã‰xito: 47
- Casos de Error: 54
- Casos de Rate Limiting: 24
- Casos LÃ­mite: 33

---

## ğŸš€ Requisitos Previos

### 1. Servidor MCP Corriendo

```bash
# Iniciar el servidor
cd /home/ubuntu/tiendanube_mcp
make start

# O con docker-compose
docker-compose up -d

# Verificar que estÃ¡ corriendo
curl http://localhost:8000/health
```

### 2. Instalar Dependencias de Prueba

```bash
# Instalar pytest, requests y locust
pip install pytest requests locust

# O usando requirements
pip install -r requirements.txt
```

---

## ğŸ§ª MÃ©todos de Prueba

### MÃ©todo 1: Script Bash (Pruebas RÃ¡pidas)

**Archivo:** `test_mcp_tools.sh`

**Ventajas:**
- No requiere dependencias Python adicionales
- RÃ¡pido y fÃ¡cil de ejecutar
- Genera reporte de resultados

**EjecuciÃ³n:**

```bash
# Hacer ejecutable
chmod +x test_mcp_tools.sh

# Ejecutar con servidor en localhost:8000
./test_mcp_tools.sh

# Ejecutar con servidor en otro host
./test_mcp_tools.sh http://192.168.1.100:8000

# Ver resultados
cat test_results_*.txt
```

**Salida Esperada:**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Suite de Pruebas - Herramientas MCP de Tienda Nube API       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Base URL: http://localhost:8000
Archivo de resultados: test_results_20250104_120000.txt

[TEST] SE-1.1: Buscar todos los endpoints de productos
[PASS] SE-1.1: HTTP 200, JSON vÃ¡lido
...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    RESUMEN DE PRUEBAS                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Pasadas:  45
â•‘ Fallidas:  2
â•‘ Omitidas:  0
â•‘ Total:    47
â•‘ Tasa de Ã©xito: 95%
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

### MÃ©todo 2: Pytest (Pruebas Completas)

**Archivo:** `test_mcp_pytest.py`

**Ventajas:**
- Pruebas organizadas por clase
- Mejor reporte de errores
- FÃ¡cil de integrar en CI/CD

**EjecuciÃ³n:**

```bash
# Ejecutar todas las pruebas
pytest test_mcp_pytest.py -v

# Ejecutar pruebas de una clase especÃ­fica
pytest test_mcp_pytest.py::TestSearchEndpoint -v

# Ejecutar una prueba especÃ­fica
pytest test_mcp_pytest.py::TestSearchEndpoint::test_se_1_1_search_all_products -v

# Ejecutar con reporte HTML
pytest test_mcp_pytest.py -v --html=report.html

# Ejecutar con cobertura
pytest test_mcp_pytest.py --cov=. --cov-report=html

# Ejecutar solo pruebas de Ã©xito
pytest test_mcp_pytest.py -k "test_.*_1_" -v

# Ejecutar solo pruebas de error
pytest test_mcp_pytest.py -k "test_.*_2_" -v

# Ejecutar solo pruebas de rendimiento
pytest test_mcp_pytest.py::TestPerformance -v
```

**Salida Esperada:**

```
test_mcp_pytest.py::TestSearchEndpoint::test_se_1_1_search_all_products PASSED
test_mcp_pytest.py::TestSearchEndpoint::test_se_1_2_search_all_orders PASSED
test_mcp_pytest.py::TestSearchEndpoint::test_se_1_3_search_post_products PASSED
...

============================== 95 passed in 12.34s ==============================
```

---

### MÃ©todo 3: Pruebas de Rate Limiting

**Archivo:** `test_rate_limiting.py`

**Ventajas:**
- Pruebas especÃ­ficas de lÃ­mites
- Simula diferentes patrones de carga
- Valida recuperaciÃ³n

**EjecuciÃ³n:**

```bash
# Ejecutar todas las pruebas de rate limiting
python test_rate_limiting.py

# Ejecutar con Locust (pruebas de carga)
locust -f test_rate_limiting.py --host=http://localhost:8000 --users 10 --spawn-rate 2
```

**Salida Esperada:**

```
======================================================================
Prueba: 5 solicitudes por segundo (dentro del lÃ­mite)
======================================================================

  [1/5] âœ“ HTTP 200 (45.2ms)
  [2/5] âœ“ HTTP 200 (42.1ms)
  [3/5] âœ“ HTTP 200 (43.8ms)
  [4/5] âœ“ HTTP 200 (44.5ms)
  [5/5] âœ“ HTTP 200 (43.9ms)

Resultado: 5/5 exitosas

======================================================================
Prueba: 15 solicitudes por segundo (excediendo el lÃ­mite)
======================================================================

  [ 1/15] âœ“ HTTP 200 (45.2ms)
  [ 2/15] âœ“ HTTP 200 (42.1ms)
  [ 3/15] âš  HTTP 429 (Rate Limited)
  ...
  [15/15] âš  HTTP 429 (Rate Limited)

Resultado: 10/15 exitosas, 5 limitadas

======================================================================
ESTADÃSTICAS DE RATE LIMITING
======================================================================

Tiempos de Respuesta:
  Promedio: 43.5ms
  MÃ­nimo:   41.2ms
  MÃ¡ximo:   125.3ms

Resultados Generales:
  Exitosas:    45
  Limitadas:   15
  Errores:     0
  Tasa de Ã©xito: 75.0%
```

---

## ğŸ“Š Casos de Prueba por Herramienta

### Herramienta 1: search_endpoint (22 casos)

```bash
# Ejecutar solo pruebas de search_endpoint
pytest test_mcp_pytest.py::TestSearchEndpoint -v
```

**Casos:**
- 6 casos de Ã©xito
- 8 casos de error
- 3 casos de rate limiting
- 5 casos lÃ­mite

---

### Herramienta 2: get_endpoint_details (27 casos)

```bash
# Ejecutar solo pruebas de get_endpoint_details
pytest test_mcp_pytest.py::TestGetEndpointDetails -v
```

**Casos:**
- 8 casos de Ã©xito
- 10 casos de error
- 3 casos de rate limiting
- 6 casos lÃ­mite

---

### Herramienta 3: get_schema (20 casos)

```bash
# Ejecutar solo pruebas de get_schema
pytest test_mcp_pytest.py::TestGetSchema -v
```

**Casos:**
- 6 casos de Ã©xito
- 7 casos de error
- 3 casos de rate limiting
- 4 casos lÃ­mite

---

### Herramienta 4: search_documentation (25 casos)

```bash
# Ejecutar solo pruebas de search_documentation
pytest test_mcp_pytest.py::TestSearchDocumentation -v
```

**Casos:**
- 8 casos de Ã©xito
- 8 casos de error
- 3 casos de rate limiting
- 6 casos lÃ­mite

---

### Herramienta 5: get_code_example (31 casos)

```bash
# Ejecutar solo pruebas de get_code_example
pytest test_mcp_pytest.py::TestGetCodeExample -v
```

**Casos:**
- 10 casos de Ã©xito
- 12 casos de error
- 3 casos de rate limiting
- 6 casos lÃ­mite

---

### Herramientas 6-8: Sin ParÃ¡metros (11 casos cada una)

```bash
# Ejecutar solo pruebas de herramientas sin parÃ¡metros
pytest test_mcp_pytest.py::TestNoParamsTools -v
```

**Casos por herramienta:**
- 3 casos de Ã©xito
- 3 casos de error
- 3 casos de rate limiting
- 2 casos lÃ­mite

---

## ğŸ” Validaciones EspecÃ­ficas

### Validar Respuestas JSON

```bash
# Ejecutar solo pruebas de validaciÃ³n JSON
pytest test_mcp_pytest.py::TestJSONValidation -v
```

**Valida:**
- JSON vÃ¡lido y bien formado
- Estructura correcta
- Campos requeridos presentes

---

### Validar Rendimiento

```bash
# Ejecutar solo pruebas de rendimiento
pytest test_mcp_pytest.py::TestPerformance -v
```

**Valida:**
- Tiempo de respuesta < 500ms
- Consistencia en tiempos

---

## ğŸ“ˆ Pruebas de Carga

### Con Apache Bench

```bash
# 1000 solicitudes, 10 concurrentes
ab -n 1000 -c 10 http://localhost:8000/tools/search_endpoint?resource=products

# 5000 solicitudes, 50 concurrentes
ab -n 5000 -c 50 http://localhost:8000/tools/search_endpoint?resource=products
```

### Con Locust

```bash
# Interfaz web
locust -f test_rate_limiting.py --host=http://localhost:8000

# LÃ­nea de comandos
locust -f test_rate_limiting.py --host=http://localhost:8000 --users 100 --spawn-rate 10 --run-time 60s --headless
```

---

## ğŸ› ï¸ Comandos Ãštiles

### Ejecutar Todas las Pruebas

```bash
# Bash
./test_mcp_tools.sh

# Pytest
pytest test_mcp_pytest.py -v

# Ambas
./test_mcp_tools.sh && pytest test_mcp_pytest.py -v
```

### Generar Reportes

```bash
# Reporte HTML con pytest
pytest test_mcp_pytest.py -v --html=report.html --self-contained-html

# Reporte con cobertura
pytest test_mcp_pytest.py --cov=. --cov-report=html --cov-report=term

# Reporte JUnit XML
pytest test_mcp_pytest.py -v --junit-xml=report.xml
```

### Ejecutar Pruebas EspecÃ­ficas

```bash
# Solo casos de Ã©xito
pytest test_mcp_pytest.py -k "test_.*_1_" -v

# Solo casos de error
pytest test_mcp_pytest.py -k "test_.*_2_" -v

# Solo casos lÃ­mite
pytest test_mcp_pytest.py -k "test_.*_3_" -v

# Solo rate limiting
pytest test_mcp_pytest.py -k "test_rate" -v

# Solo una herramienta
pytest test_mcp_pytest.py::TestSearchEndpoint -v
```

### Ejecutar en Paralelo

```bash
# Instalar pytest-xdist
pip install pytest-xdist

# Ejecutar con 4 workers
pytest test_mcp_pytest.py -v -n 4
```

---

## ğŸ“‹ Checklist de Pruebas

### Antes de Ejecutar

- [ ] Servidor MCP corriendo en http://localhost:8000
- [ ] Dependencias instaladas (pytest, requests, locust)
- [ ] Base de datos de API cargada
- [ ] Health check pasando (`curl http://localhost:8000/health`)

### Durante la EjecuciÃ³n

- [ ] Monitorear logs del servidor
- [ ] Verificar uso de CPU y memoria
- [ ] Anotar cualquier comportamiento inusual

### DespuÃ©s de Ejecutar

- [ ] Revisar reporte de resultados
- [ ] Documentar fallos
- [ ] Crear issues si hay problemas
- [ ] Archivar resultados para referencia

---

## ğŸ› Troubleshooting

### Error: Connection refused

```
Error: No se puede conectar a http://localhost:8000
```

**SoluciÃ³n:**
```bash
# Verificar que el servidor estÃ¡ corriendo
curl http://localhost:8000/health

# Si no estÃ¡ corriendo, iniciar
make start
```

### Error: Timeout

```
Error: Timeout esperando respuesta
```

**SoluciÃ³n:**
```bash
# Aumentar timeout en scripts
# En test_mcp_pytest.py, cambiar TIMEOUT = 10 a TIMEOUT = 30

# O ejecutar con menos concurrencia
pytest test_mcp_pytest.py -v -n 1
```

### Error: Rate limiting no detectado

```
Error: Prueba de rate limiting fallida
```

**SoluciÃ³n:**
```bash
# Verificar configuraciÃ³n de rate limiting en nginx.conf
# Verificar que Nginx estÃ¡ corriendo
docker ps | grep nginx

# Revisar logs de Nginx
docker logs tiendanube-mcp-nginx
```

### Error: JSON invÃ¡lido

```
Error: JSON parsing error
```

**SoluciÃ³n:**
```bash
# Verificar respuesta manualmente
curl -s http://localhost:8000/tools/search_endpoint?resource=products | python -m json.tool

# Si hay error, revisar logs del servidor
docker logs tiendanube-mcp-server
```

---

## ğŸ“Š InterpretaciÃ³n de Resultados

### Tasa de Ã‰xito

| Tasa | InterpretaciÃ³n |
|------|---|
| 100% | âœ“ Perfecto - Todos los tests pasan |
| 95-99% | âœ“ Excelente - Fallos menores |
| 90-94% | âš  Bueno - Revisar fallos |
| 80-89% | âš  Aceptable - Investigar problemas |
| <80% | âœ— CrÃ­tico - Resolver antes de producciÃ³n |

### Tiempo de Respuesta

| Tiempo | InterpretaciÃ³n |
|--------|---|
| <100ms | âœ“ Excelente |
| 100-200ms | âœ“ Bueno |
| 200-500ms | âš  Aceptable |
| >500ms | âœ— Lento - Investigar |

### Rate Limiting

| Comportamiento | Esperado |
|---|---|
| 0-10 req/s | âœ“ Todas exitosas |
| 10-15 req/s | âš  Algunas limitadas |
| >15 req/s | âœ“ MayorÃ­a limitadas |

---

## ğŸ“ DocumentaciÃ³n de Resultados

### Formato de Reporte

```
Fecha: 2025-01-04
Hora: 12:00:00
Servidor: http://localhost:8000
VersiÃ³n: 1.0.0

RESUMEN:
  Total Casos: 158
  Pasadas: 150
  Fallidas: 8
  Omitidas: 0
  Tasa de Ã‰xito: 94.9%

DETALLES POR HERRAMIENTA:
  search_endpoint: 21/22 (95.5%)
  get_endpoint_details: 26/27 (96.3%)
  get_schema: 20/20 (100%)
  search_documentation: 24/25 (96%)
  get_code_example: 30/31 (96.8%)
  get_authentication_info: 11/11 (100%)
  get_multi_inventory_info: 11/11 (100%)
  list_resources: 11/11 (100%)

FALLOS:
  - SE-2.4: Query vacÃ­o (esperado 400, obtuvo 200)
  - GED-2.4: Path invÃ¡lido (esperado 404, obtuvo 200)
  ...

RECOMENDACIONES:
  - Revisar validaciÃ³n de parÃ¡metros
  - Mejorar manejo de errores
  - Optimizar rendimiento
```

---

## ğŸš€ IntegraciÃ³n en CI/CD

### GitHub Actions

```yaml
name: Test MCP Tools

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        pip install pytest requests locust
    
    - name: Start server
      run: |
        docker-compose up -d
        sleep 5
    
    - name: Run tests
      run: |
        pytest test_mcp_pytest.py -v --junit-xml=report.xml
    
    - name: Upload report
      uses: actions/upload-artifact@v2
      if: always()
      with:
        name: test-report
        path: report.xml
```

---

**VersiÃ³n:** 1.0.0  
**Ãšltima actualizaciÃ³n:** 2025-01-04  
**Total Casos de Prueba:** 158
